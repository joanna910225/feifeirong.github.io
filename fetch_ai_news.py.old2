import requests
import json
import os
from datetime import datetime, timedelta, timezone

# API details (get your key from https://x.ai/api)
API_URL = "https://api.x.ai/v1/chat/completions"  # Adjust if different
API_KEY = os.getenv("XAI_API_KEY")

# Dynamic date for 24h ago
now = datetime.now(timezone.utc)
yesterday = (now - timedelta(days=1)).strftime("%Y-%m-%d")

# Generate timestamp for file name (e.g., 2025-09-24_12-09-33)
timestamp = now.strftime("%Y-%m-%d_%H-%M-%S")
output_folder = "news"
os.makedirs(output_folder, exist_ok=True)  # Create 'news' folder if it doesn't exist
output_file = os.path.join(output_folder, f"grok_news_summary_{timestamp}.json")

system_prompt = f"""You are an AI news curator specializing in artificial intelligence and technology. Your task is to provide concise, accurate summaries of the most significant developments from the past 24 hours (from utc {yesterday} to now). If data is sparse within this window, include notable recent developments (e.g., from the past week) and clearly note their dates. Focus on key areas like new model releases (e.g., LLMs, vision models, MoE architectures), research papers (e.g., arXiv uploads, conference preprints), open-source projects (e.g., GitHub repos, Hugging Face models/datasets), tools, software updates, major announcements from companies or organizations, and general AI news including big AI tech firm actions and breakthroughs.

Always use tools to gather real-time data, tailoring queries and sources to each response section for comprehensiveness:
- **For Model Releases and Updates**: Use web_search or web_search_with_snippets with queries like "new AI model releases past 24 hours site:huggingface.co/models OR site:modelscope.cn/models OR site:openai.com/blog OR site:ai.meta.com/blog OR site:deepmind.google OR site:anthropic.com" to find open-source and proprietary releases. List all popular ones (e.g., high downloads/stars, from major orgs like OpenAI, Meta, Alibaba). Chain with browse_page on specific model pages or announcement URLs (instructions: "Extract release date, key features, impact, and links; confirm if within past 24 hours"). Cross-verify with x_keyword_search or x_semantic_search on X for discussions (e.g., query: "new AI model release since:{yesterday} until:{now} min_faves:50 filter:has_engagement"). If no recent hits, broaden to "past week".
- **For New Research Papers**: Use web_search_with_snippets or web_search with queries like "arXiv AI papers uploaded {yesterday} site:arxiv.org" or "new ML preprints past day site:arxiv.org/list/cs/recent OR site:arxiv.org/list/stat/recent". If needed, browse_page on https://arxiv.org/list/cs/recent (instructions: "List papers submitted in past 24 hours with titles, authors, abstracts, and links; focus on AI/tech categories like cs.LG, cs.AI"). Include bio/tech overlaps from site:biorxiv.org or site:paperswithcode.com if relevant. If sparse, include recent papers from the past week and note dates. Present in a table format.
- **For Open-Source Projects and Tools**: Use web_search with queries like "trending AI GitHub repositories created after {yesterday} site:github.com/trending" or "new open-source AI tools past 24 hours site:huggingface.co/spaces OR site:pypi.org". Browse_page on https://github.com/trending/python?since=daily (instructions: "Extract new repos/tools with stars >50, descriptions, impacts, and links; filter for AI/tech"). Supplement with x_keyword_search on X (e.g., query: "new open-source AI project GitHub since:{yesterday} min_faves:50"). If limited, expand to past week trends.
- **For General AI News (including big AI tech firm actions and breakthroughs)**: Use web_search or web_search_with_snippets with queries like "AI news breakthroughs past 24 hours site:techcrunch.com OR site:venturebeat.com OR site:reuters.com/tech OR site:nytimes.com/technology OR site:theverge.com/ai" to capture major events, partnerships, investments, regulatory actions, or scientific breakthroughs from big tech firms (e.g., Google, Microsoft, Amazon, NVIDIA). Supplement with x_keyword_search or x_semantic_search on X (e.g., query: "AI breakthrough OR big AI announcement since:{yesterday} min_faves:100 filter:news"). Chain browse_page on company blogs (e.g., https://blog.google/technology/ai/) with instructions: "Summarize key actions, impacts, and links; focus on verifiable breakthroughs or firm-specific news within 24 hours". If sparse, include recent news from the past week and note accordingly.

If results are limited, chain tools: Follow up on links from initial searches with browse_page for detailed summaries. Cross-verify claims from social media with official sources. Use operators like since:{yesterday} until:{now}, min_faves:50, filter:has_engagement in X searches for high-impact items.

Structure your response as:
- A brief intro with timestamp.
- Sections: Model Releases and Updates, New Research Papers (in a table), Open-Source Projects and Tools, General AI News (a paragraph summarizing big AI tech firm actions, breakthroughs, and other notable events).
- Bulleted or tabulated items with key details, impact, and verifiable links.
- Inline citations via render components if applicable.

Prioritize objectivity, avoid hype, and note if info is unverified. If data is sparse, explain, suggest official checks, and include recent alternatives with their dates. Ensure information is up-to-date as of the query time. Do not assume dates are in the future; use the provided dates exactly."""

user_prompt = f"""Summarize the most significant artificial intelligence and technology developments in the past 24 hours (from {yesterday} to now), including new tools, updates, and announcements. Focus on model releases, new papers, and open-source projects, and provide relevant links."""

# Initial messages
messages = [
    {"role": "system", "content": system_prompt},
    {"role": "user", "content": user_prompt}
]

payload = {
    "model": "grok-4",  # Or "grok-3"
    "messages": messages,
    "temperature": 0.5,
    "max_tokens": 3000,
    "tools": [  # Define tools with proper JSON schema for parameters
        {
            "type": "function",
            "function": {
                "name": "web_search",
                "description": "Search the web for information.",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "query": {"type": "string", "description": "The search query."},
                        "num_results": {"type": "integer", "description": "Number of results, default 10, max 30."}
                    },
                    "required": ["query"]
                }
            }
        },
        {
            "type": "function",
            "function": {
                "name": "x_keyword_search",
                "description": "Advanced search for X posts.",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "query": {"type": "string", "description": "The search query with operators."},
                        "limit": {"type": "integer", "description": "Number of posts, default 10."},
                        "mode": {"type": "string", "enum": ["Top", "Latest"], "description": "Sort mode, default Top."}
                    },
                    "required": ["query"]
                }
            }
        },
        {
            "type": "function",
            "function": {
                "name": "x_semantic_search",
                "description": "Semantic search for X posts.",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "query": {"type": "string", "description": "Semantic query."},
                        "limit": {"type": "integer", "description": "Number of posts, default 10."},
                        "from_date": {"type": "string", "description": "From date YYYY-MM-DD."},
                        "to_date": {"type": "string", "description": "To date YYYY-MM-DD."},
                        "exclude_usernames": {"type": "array", "items": {"type": "string"}, "description": "Exclude usernames."},
                        "usernames": {"type": "array", "items": {"type": "string"}, "description": "Include usernames."},
                        "min_score_threshold": {"type": "number", "description": "Min relevancy score, default 0.18."}
                    },
                    "required": ["query"]
                }
            }
        },
        {
            "type": "function",
            "function": {
                "name": "browse_page",
                "description": "Browse a webpage and summarize based on instructions.",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "url": {"type": "string", "description": "The URL to browse."},
                        "instructions": {"type": "string", "description": "Instructions for summarization."}
                    },
                    "required": ["url", "instructions"]
                }
            }
        }
        # Add more tools as needed, following the same pattern
    ],
    "tool_choice": "auto"  # Encourage automatic tool use
}

headers = {
    "Authorization": f"Bearer {API_KEY}",
    "Content-Type": "application/json"
}

# Function to execute tools (placeholder: implement actual tool logic here)
def execute_tool(function_name, args):
    # This is where you implement the tool calls client-side.
    # For example, use external libraries/APIs to perform the search.
    # Below are dummy implementations; replace with real ones (e.g., use serpapi for web_search, twitter API for x_search).
    if function_name == "web_search":
        # Dummy: Simulate web search result
        return {"results": ["Dummy web search result for query: " + args.get("query", "")]}
    elif function_name == "x_keyword_search":
        # Dummy: Simulate X post search
        return {"posts": ["Dummy X post for query: " + args.get("query", "")]}
    elif function_name == "x_semantic_search":
        return {"posts": ["Dummy semantic X post for query: " + args.get("query", "")]}
    elif function_name == "browse_page":
        # Dummy: Simulate page browse
        return {"summary": "Dummy summary for URL: " + args.get("url", "") + " with instructions: " + args.get("instructions", "")}
    else:
        return {"error": "Unknown tool"}

# Agent loop to handle tool calls (up to 5 iterations to prevent infinite loops)
max_iterations = 5
for _ in range(max_iterations):
    response = requests.post(API_URL, headers=headers, data=json.dumps(payload))
    if response.status_code != 200:
        print(f"Error: {response.status_code} - {response.text}")
        break

    result = response.json()
    if 'choices' not in result or not result['choices']:
        print("No choices in response")
        break

    message = result['choices'][0]['message']
    messages.append(message)  # Add assistant's message to conversation

    if 'tool_calls' not in message:
        # No more tool calls; this is the final response
        final_content = message['content']
        # Save the final summary to JSON file
        save_data = {
            "timestamp": timestamp,
            "summary": final_content
        }
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(save_data, f, ensure_ascii=False, indent=4)
        print(f"Summary saved to {output_file}")
        print(final_content)
        break

    # Handle tool calls
    tool_calls = message['tool_calls']
    for tool_call in tool_calls:
        function_name = tool_call['function']['name']
        args_str = tool_call['function']['arguments']
        args = json.loads(args_str)
        print(f"Executing tool: {function_name} with args {args}")
        
        # Execute the tool and get result
        tool_result = execute_tool(function_name, args)
        
        # Append tool response as a new message
        messages.append({
            "role": "tool",
            "content": json.dumps(tool_result),
            "tool_call_id": tool_call['id']  # Required for some APIs
        })

    # Update payload for next iteration with accumulated messages
    payload["messages"] = messages

else:
    print("Max iterations reached without final response")
